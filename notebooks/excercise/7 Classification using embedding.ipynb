{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T19:26:01.063156Z",
     "start_time": "2019-03-16T19:25:49.514927Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Deciding which embedding to use\n",
    "possible_word_vectors = (50, 100, 200, 300)\n",
    "word_vectors = possible_word_vectors[0]\n",
    "\n",
    "# TODO: download from here\n",
    "# https://github.com/stanfordnlp/GloVe\n",
    "# specifically: http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
    "file_name = f'glove.6B.{word_vectors}d.txt'\n",
    "\n",
    "filepath = '/Users/nickager/Downloads/glove/'\n",
    "pretrained_embedding = os.path.join(filepath, file_name)\n",
    "\n",
    "    \n",
    "embeddings_index = {}\n",
    "with open(pretrained_embedding, \"rb\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0].decode(\"utf-8\") \n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen = [ 0.37854    1.8233    -1.2648    -0.1043     0.35829    0.60029\n",
      " -0.17538    0.83767   -0.056798  -0.75795    0.22681    0.98587\n",
      "  0.60587   -0.31419    0.28877    0.56013   -0.77456    0.071421\n",
      " -0.5741     0.21342    0.57674    0.3868    -0.12574    0.28012\n",
      "  0.28135   -1.8053    -1.0421    -0.19255   -0.55375   -0.054526\n",
      "  1.5574     0.39296   -0.2475     0.34251    0.45365    0.16237\n",
      "  0.52464   -0.070272  -0.83744   -1.0326     0.45946    0.25302\n",
      " -0.17837   -0.73398   -0.20025    0.2347    -0.56095   -2.2839\n",
      "  0.0092753 -0.60284  ]\n",
      "predicted_queen =  [ 0.417366    0.90427    -1.00503    -0.062021    0.49726     0.80667\n",
      " -0.14855     0.80365    -0.15654    -0.66974     0.234354    0.62476\n",
      "  0.925871   -0.971       0.92566     0.89915    -1.54596    -0.52625\n",
      "  0.136954    0.66199     0.48716     0.37035    -0.214214    0.10101\n",
      "  0.71358    -2.0875     -1.1362     -1.14961    -0.53599     0.2739\n",
      "  1.6723      0.02931    -0.77656     0.46056286  0.34866    -0.057417\n",
      "  0.19444    -0.207748   -0.73039    -0.10752     0.235544    0.96424\n",
      " -0.46994    -0.487275   -0.254       0.46213    -0.66081    -1.94515\n",
      " -0.68797    -0.49784   ]\n",
      "diffence =  [-0.038826    0.91903    -0.25977    -0.042279   -0.13897    -0.20638\n",
      " -0.02683     0.03402     0.099742   -0.08821    -0.007544    0.36111\n",
      " -0.320001    0.65681    -0.63689    -0.33902     0.7714      0.597671\n",
      " -0.711054   -0.44857     0.08958     0.01645     0.088474    0.17911\n",
      " -0.43223     0.2822      0.0941      0.95706    -0.01776    -0.328426\n",
      " -0.1149      0.36365     0.52906    -0.11805286  0.10499     0.219787\n",
      "  0.3302      0.137476   -0.10705    -0.92508     0.223916   -0.71122\n",
      "  0.29157    -0.246705    0.05375    -0.22743     0.09986    -0.33875\n",
      "  0.6972453  -0.105     ]\n"
     ]
    }
   ],
   "source": [
    "# taken from: /Users/nickager/Downloads/glove/glove.6B.50d.txt\n",
    "\"king 0.50451 0.68607 -0.59517 -0.022801 0.60046 -0.13498 -0.08813 0.47377 -0.61798 -0.31012 -0.076666 1.493 -0.034189 -0.98173 0.68229 0.81722 -0.51874 -0.31503 -0.55809 0.66421 0.1961 -0.13495 -0.11476 -0.30344 0.41177 -2.223 -1.0756 -1.0783 -0.34354 0.33505 1.9927 -0.04234 -0.64319 0.71125 0.49159 0.16754 0.34344 -0.25663 -0.8523 0.1661 0.40102 1.1685 -1.0137 -0.21585 -0.15155 0.78321 -0.91241 -1.6106 -0.64426 -0.51042\".replace(\" \", \",\")\n",
    "king = np.asarray([0.50451,0.68607,-0.59517,-0.022801,0.60046,-0.13498,-0.08813,0.47377,-0.61798,-0.31012,-0.076666,1.493,-0.034189,-0.98173,0.68229,0.81722,-0.51874,-0.31503,-0.55809,0.66421,0.1961,-0.13495,-0.11476,-0.30344,0.41177,-2.223,-1.0756,-1.0783,-0.34354,0.33505,1.9927,-0.04234,-0.64319,0.71125,0.49159,0.16754,0.34344,-0.25663,-0.8523,0.1661,0.40102,1.1685,-1.0137,-0.21585,-0.15155,0.78321,-0.91241,-1.6106,-0.64426,-0.51042])\n",
    "\"queen 0.37854 1.8233 -1.2648 -0.1043 0.35829 0.60029 -0.17538 0.83767 -0.056798 -0.75795 0.22681 0.98587 0.60587 -0.31419 0.28877 0.56013 -0.77456 0.071421 -0.5741 0.21342 0.57674 0.3868 -0.12574 0.28012 0.28135 -1.8053 -1.0421 -0.19255 -0.55375 -0.054526 1.5574 0.39296 -0.2475 0.34251 0.45365 0.16237 0.52464 -0.070272 -0.83744 -1.0326 0.45946 0.25302 -0.17837 -0.73398 -0.20025 0.2347 -0.56095 -2.2839 0.0092753 -0.60284\".replace(\" \", \",\")\n",
    "queen = np.asarray([0.37854,1.8233,-1.2648,-0.1043,0.35829,0.60029,-0.17538,0.83767,-0.056798,-0.75795,0.22681,0.98587,0.60587,-0.31419,0.28877,0.56013,-0.77456,0.071421,-0.5741,0.21342,0.57674,0.3868,-0.12574,0.28012,0.28135,-1.8053,-1.0421,-0.19255,-0.55375,-0.054526,1.5574,0.39296,-0.2475,0.34251,0.45365,0.16237,0.52464,-0.070272,-0.83744,-1.0326,0.45946,0.25302,-0.17837,-0.73398,-0.20025,0.2347,-0.56095,-2.2839,0.0092753,-0.60284])\n",
    "\"man -0.094386 0.43007 -0.17224 -0.45529 1.6447 0.40335 -0.37263 0.25071 -0.10588 0.10778 -0.10848 0.15181 -0.65396 0.55054 0.59591 -0.46278 0.11847 0.64448 -0.70948 0.23947 -0.82905 1.272 0.033021 0.2935 0.3911 -2.8094 -0.70745 0.4106 0.3894 -0.2913 2.6124 -0.34576 -0.16832 0.25154 0.31216 0.31639 0.12539 -0.012646 0.22297 -0.56585 -0.086264 0.62549 -0.0576 0.29375 0.66005 -0.53115 -0.48233 -0.97925 0.53135 -0.11725\".replace(\" \", \",\")\n",
    "man = np.asarray([-0.094386,0.43007,-0.17224,-0.45529,1.6447,0.40335,-0.37263,0.25071,-0.10588,0.10778,-0.10848,0.15181,-0.65396,0.55054,0.59591,-0.46278,0.11847,0.64448,-0.70948,0.23947,-0.82905,1.272,0.033021,0.2935,0.3911,-2.8094,-0.70745,0.4106,0.3894,-0.2913,2.6124,-0.34576,-0.16832,0.25154,0.31216,0.31639,0.12539,-0.012646,0.22297,-0.56585,-0.086264,0.62549,-0.0576,0.29375,0.66005,-0.53115,-0.48233,-0.97925,0.53135,-0.11725])\n",
    "\"woman -0.18153 0.64827 -0.5821 -0.49451 1.5415 1.345 -0.43305 0.58059 0.35556 -0.25184 0.20254 -0.71643 0.3061 0.56127 0.83928 -0.38085 -0.90875 0.43326 -0.014436 0.23725 -0.53799 1.7773 -0.066433 0.69795 0.69291 -2.6739 -0.76805 0.33929 0.19695 -0.35245 2.292 -0.27411 -0.30169 0.00085286 0.16923 0.091433 -0.02361 0.036236 0.34488 -0.83947 -0.25174 0.42123 0.48616 0.022325 0.5576 -0.85223 -0.23073 -1.3138 0.48764 -0.10467\".replace(\" \", \",\")\n",
    "woman = np.asarray([-0.18153,0.64827,-0.5821,-0.49451,1.5415,1.345,-0.43305,0.58059,0.35556,-0.25184,0.20254,-0.71643,0.3061,0.56127,0.83928,-0.38085,-0.90875,0.43326,-0.014436,0.23725,-0.53799,1.7773,-0.066433,0.69795,0.69291,-2.6739,-0.76805,0.33929,0.19695,-0.35245,2.292,-0.27411,-0.30169,0.00085286,0.16923,0.091433,-0.02361,0.036236,0.34488,-0.83947,-0.25174,0.42123,0.48616,0.022325,0.5576,-0.85223,-0.23073,-1.3138,0.48764,-0.10467])\n",
    "\n",
    "predicted_queen = king - man + woman\n",
    "\n",
    "diffence = queen - predicted_queen\n",
    "print(\"queen =\",queen)\n",
    "print(\"predicted_queen = \", predicted_queen)\n",
    "print(\"diffence = \", diffence)\n",
    "# I guess it might be closer with a longer encoding eg \"glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:38:30.071438Z",
     "start_time": "2019-03-16T20:38:29.453834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting the data\n",
    "cats = ['alt.atheism', 'sci.space']\n",
    "# After try a multiclass example\n",
    "# cats = ['alt.atheism', 'talk.religion.misc',\n",
    "#         'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=cats)\n",
    "\n",
    "X_train = newsgroups_train['data']\n",
    "y_train = newsgroups_train['target']\n",
    "\n",
    "X_test = newsgroups_test['data']\n",
    "y_test = newsgroups_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:38:37.755700Z",
     "start_time": "2019-03-16T20:38:37.745110Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-74c05b0f5b11>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-74c05b0f5b11>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    embedded_docs.append(np.mean(embedded_document, axis=0))\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    \"\"\"\n",
    "    Follows the scikit-learn API\n",
    "    Transform each document in the average\n",
    "    of the embeddings of the words in it\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = 50\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Find the embedding vector for each word in the dictionary\n",
    "        and take the mean for each document\n",
    "        \"\"\"\n",
    "        # Renaming it just to make it more understandable \n",
    "        documents = X\n",
    "        embedded_docs = []\n",
    "        for document in documents:\n",
    "            # For each document\n",
    "            # Consider the mean of all the embeddings\n",
    "            embedded_document = []\n",
    "            for words in document:\n",
    "\n",
    "                for w in words:\n",
    "                    if w in self.word2vec:\n",
    "                        embedded_word = self.word2vec[w]\n",
    "                    else:\n",
    "                        embedded_word = np.zeros(self.dim)\n",
    "                    embedded_document.append(embedded_word\n",
    "            embedded_docs.append(np.mean(embedded_document, axis=0))\n",
    "\n",
    "        return embedded_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:38:35.091007Z",
     "start_time": "2019-03-16T20:38:30.082634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the embedding\n",
    "e = EmbeddingVectorizer(embeddings_index)\n",
    "X_train_embedded = e.transform(X_train)\n",
    "\n",
    "# Train the classifier\n",
    "rf = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "rf.fit(X_train_embedded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:38:37.721048Z",
     "start_time": "2019-03-16T20:38:35.097789Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_embedded = e.transform(X_test)\n",
    "predictions = rf.predict(X_test_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:39:17.559300Z",
     "start_time": "2019-03-16T20:39:17.545871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score:  0.7405204936377006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[224,  88],\n",
       "       [ 95, 306]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('AUC score: ', roc_auc_score(predictions, y_test))\n",
    "confusion_matrix(predictions, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
